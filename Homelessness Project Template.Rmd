---
title: "Homelessness"
output: html_document
editor_options: 
  chunk_output_type: inline
---
## Introduction

The 2020 [point-in-time count](https://www.kingcounty.gov/elected/executive/constantine/news/release/2020/July/01-homeless-count.aspx) of people experiencing homelessness for Seattle/King County was 11,751. This represents a 5% increase over the 2019 count and reflects similar trend across many counties in the western U.S. A step towards addressing homelessness is improving our understanding of the relationship between local housing market factors and homelessness. 

The U.S. Department of Housing and Urban Development (HUD) produced a report in 2019 [Market Predictors of Homelessness](https://www.huduser.gov/portal/sites/default/files/pdf/Market-Predictors-of-Homelessness.pdf) that describes a model-based approach to understanding of the relationship between local housing market factors, policies, demographics, climate and homelessness. Our project is motivated by the goals of the HUD study:

"To continue progressing toward the goals of ending and preventing homelessness, we must further our knowledge of the basic community-level determinants of homelessness. The primary objectives of this study are to (1) identify market factors that have established effects on homelessness, (2) construct and evaluate empirical models of community-level homelessness.."

We will investigate whether there are alternative modeling approaches that outperform the models described in the HUD report.

## Data Collection

The data for this project are described in HUD's report [Market Predictors of Homelessness](https://www.huduser.gov/portal/sites/default/files/pdf/Market-Predictors-of-Homelessness.pdf) in the section titled DATA.

I will refer you to this section of the HUD report for a detailed description of the sources of the data and how they were processed.


### Load necessary packages

```{r}

#skimr provides a nice summary of a data set
library(skimr)
#leaps will be used for model selection
library(leaps)
#readxl lets us read Excel files
library(readxl)
#GGally has a nice pairs plotting function
library(GGally)
#corrplot has nice plots for correlation matrices
library(corrplot)
#gridExtra
library(gridExtra)
#glmnet is used to fit glm's. It will be used for lasso and ridge regression models.
library(glmnet)
#tidymodels has a nice workflow for many models. We will use it for XGBoost
library(tidymodels)
#xgboost lets us fit XGBoost models
library(xgboost)
#vip is used to visualize the importance of predicts in XGBoost models
library(vip)
#tidyverse contains packages we will use for processing and plotting data
library(tidyverse)

#Set the plotting theme
theme_set(theme_bw())

```


### Examine the data dictionary

The data dictionary `HUD TO3 - 05b Analysis File - Data Dictionary.xlsx` contains descriptions of all variables in the data set.

$\rightarrow$ Load the data dictionary (call it `dictionary`) and view its contents using the function `View`.

```{r}

dictionary <- read_xlsx("HUD TO3 - 05b Analysis File - Data Dictionary.xlsx")

```


#### What are the sources of data?

$\rightarrow$ Use the dictionary to find the unique sources of data that are not derived from other variables. We will assume that derived variables are either have `Derived` equal to `No` or `Source or Root Variable` starts with the string `See`.

```{r}

dictionary %>% 
  filter(Derived == "No" & str_detect(`Source or Root Variable`, "See") == FALSE) %>%  
  select(`Source or Root Variable`) %>% 
  unique()

```

What are the sources of most of the data?

$\rightarrow$ Make a bar graph of the counts of different data sources described in `Source or Root Variable`. Your graph should have the following features:

1. Order the bars in descending order based on the count.
2. Only include the 10 most common data sources.
3. Orient the plot so that it is easy to read the labels.

```{r}

dictionary %>% 
  count(`Source or Root Variable`) %>% 
  mutate(`Source or Root Variable` = fct_reorder(`Source or Root Variable`, n)) %>% 
  head(10) %>% 
  ggplot(aes(x = `Source or Root Variable`, y = n)) +
  labs(x = "Source or Root Variable", y = "Count") +
  geom_col() +
  coord_flip()

```


$\rightarrow$ What are the different Associated Domains of the variables?

```{r}

dictionary %>% 
  count(`Associated Domain`) %>% 
  mutate(`Associated Domain` = fct_reorder(`Associated Domain`, n)) %>% 
  ggplot(aes(x = `Associated Domain`, y = n/sum(n))) +
  geom_col() +
  scale_y_continuous(labels = scales::percent) +
  labs(x = "Associated Domain", y = "Percentage") +
  coord_flip()

```

A big part of this project will be figuring out what variables to include in the analysis.


### Data ethics


#### Data Science Ethics Checklist

[![Deon badge](https://img.shields.io/badge/ethics%20checklist-deon-brightgreen.svg?style=popout-square)](http://deon.drivendata.org/)

**A. Problem Formulation**

 - [ ] **A.1 Well-Posed Problem**: Is it possible to answer our question with data? Is the problem well-posed?

**B. Data Collection**

 - [ ] **B.1 Informed consent**: If there are human subjects, have they given informed consent, where subjects affirmatively opt-in and have a clear understanding of the data uses to which they consent?
 - [ ] **B.2 Collection bias**: Have we considered sources of bias that could be introduced during data collection and survey design and taken steps to mitigate those?
 - [ ] **B.3 Limit PII exposure**: Have we considered ways to minimize exposure of personally identifiable information (PII) for example through anonymization or not collecting information that isn't relevant for analysis?
 - [ ] **B.4 Downstream bias mitigation**: Have we considered ways to enable testing downstream results for biased outcomes (e.g., collecting data on protected group status like race or gender)?

**C. Data Storage**

 - [ ] **C.1 Data security**: Do we have a plan to protect and secure data (e.g., encryption at rest and in transit, access controls on internal users and third parties, access logs, and up-to-date software)?
 - [ ] **C.2 Right to be forgotten**: Do we have a mechanism through which an individual can request their personal information be removed?
 - [ ] **C.3 Data retention plan**: Is there a schedule or plan to delete the data after it is no longer needed?

**D. Analysis**

 - [ ] **D.1 Missing perspectives**: Have we sought to address blindspots in the analysis through engagement with relevant stakeholders (e.g., checking assumptions and discussing implications with affected communities and subject matter experts)?
 - [ ] **D.2 Dataset bias**: Have we examined the data for possible sources of bias and taken steps to mitigate or address these biases (e.g., stereotype perpetuation, confirmation bias, imbalanced classes, or omitted confounding variables)?
 - [ ] **D.3 Honest representation**: Are our visualizations, summary statistics, and reports designed to honestly represent the underlying data?
 - [ ] **D.4 Privacy in analysis**: Have we ensured that data with PII are not used or displayed unless necessary for the analysis?
 - [ ] **D.5 Auditability**: Is the process of generating the analysis well documented and reproducible if we discover issues in the future?

**E. Modeling**

 - [ ] **E.1 Proxy discrimination**: Have we ensured that the model does not rely on variables or proxies for variables that are unfairly discriminatory?
 - [ ] **E.2 Fairness across groups**: Have we tested model results for fairness with respect to different affected groups (e.g., tested for disparate error rates)?
 - [ ] **E.3 Metric selection**: Have we considered the effects of optimizing for our defined metrics and considered additional metrics?
 - [ ] **E.4 Explainability**: Can we explain in understandable terms a decision the model made in cases where a justification is needed?
 - [ ] **E.5 Communicate bias**: Have we communicated the shortcomings, limitations, and biases of the model to relevant stakeholders in ways that can be generally understood?

**F. Deployment**

 - [ ] **F.1 Redress**: Have we discussed with our organization a plan for response if users are harmed by the results (e.g., how does the data science team evaluate these cases and update analysis and models to prevent future harm)?
 - [ ] **F.2 Roll back**: Is there a way to turn off or roll back the model in production if necessary?
 - [ ] **F.3 Concept drift**: Do we test and monitor for concept drift to ensure the model remains fair over time?
 - [ ] **F.4 Unintended use**: Have we taken steps to identify and prevent unintended uses and abuse of the model and do we have a plan to monitor these once the model is deployed?

*Data Science Ethics Checklist generated with [deon](http://deon.drivendata.org).*


We will discuss these issues in class.


## Data Preparation



### Load the data 


The HUD data set is contained in the file `05b_analysis_file_update.csv`.


$\rightarrow$ Load the data set contained in the file `05b_analysis_file_update.csv` and name the data frame `df`.

```{r}

df <- read_csv("05b_analysis_file_update.csv")

```

### Explore the contents of the data set


$\rightarrow$ Look at the first few rows of the data frame. 

```{r}

head(df)

```


Refer to the data dictionary and the [HUD report]((https://www.huduser.gov/portal/sites/default/files/pdf/Market-Predictors-of-Homelessness.pdf)) to understand what variables are present.


#### Explore the columns

What are the variables?

What variable(s) do we want to predict?

What variables seem useful as predictors?

What predictor variables are redundant?


It will take a significant amount of work to understand the contents of the data set. We will discuss this in class.

#### Select a subset of columns

Below are suggested variables to keep in the analysis. You may include other variables that might be useful as predictors. Provide an explanation for why you kept the variables.

$\rightarrow$ Construct the list `variable_names` that we will keep for the analysis. 

```{r}
#Search through data dictionary to find other variables to include


variable_names <- c("year", "cocnumber",
  
  "pit_tot_hless_pit_hud", "pit_tot_shelt_pit_hud", "pit_tot_unshelt_pit_hud","dem_pop_pop_census",

  "fhfa_hpi_2009", "ln_hou_mkt_medrent_xt", "hou_mkt_utility_xt", "hou_mkt_burden_own_acs5yr_2017", "hou_mkt_burden_sev_rent_acs_2017", "hou_mkt_rentshare_acs5yr_2017", "hou_mkt_rentvacancy_xt", "hou_mkt_density_dummy", "hou_mkt_evict_count", "hou_mkt_ovrcrowd_acs5yr_2017", "major_city", "suburban",
           
           "econ_labor_unemp_rate_BLS", "econ_labor_incineq_acs5yr_2017", "econ_labor_pov_pop_census_share",
           
           "hou_pol_hudunit_psh_hud_share", "hou_pol_occhudunit_psh_hud", "hou_mkt_homeage1940_xt",
           
           "dem_soc_black_census", "dem_soc_hispanic_census", "dem_soc_asian_census", "dem_soc_pacific_census", "dem_pop_child_census", "dem_pop_senior_census", "dem_pop_female_census", "dem_pop_mig_census", "d_dem_pop_mig_census_share", "dem_soc_singadult_xt", "dem_soc_singparent_xt", "dem_soc_vet_xt", "dem_soc_ed_lessbach_xt", "dem_health_cost_dart", 
           "dem_health_excesdrink_chr",
           
           "env_wea_avgtemp_noaa", "env_wea_avgtemp_summer_noaa", "env_wea_precip_noaa", "env_wea_precip_annual_noaa",
          "dem_soc_ed_bach_acs5yr", "dem_soc_ed_hsgrad_acs5yr", "dem_soc_ed_lesshs_acs5yr", "dem_soc_ed_somecoll_acs5yr")
           
```

I've added some extra variables that looked at the education level of each state to see if these variables would improve the overall model or not.

$\rightarrow$ Select this subset of variables from the full data set. Call the new data frame `df_small`.

```{r}

df_small <- df %>% 
  select(all_of(variable_names))

```

$\rightarrow$ Examine the head of the new, smaller data frame.

```{r}

head(df_small)

```

$\rightarrow$ Create a new dictionary for this subset of variables and use `View` to examine the contents.

```{r}

#Create the smaller data dictionary
dictionary_small <- dictionary %>% 
  filter(Variable %in% variable_names) 

#View it
dictionary_small %>% 
  View()

```

How many variables of each Associated Domain are in the smaller data set?

$\rightarrow$ Make a bar graph of the counts of different `Associated Domain`. Your graph should have the following features:

1. Order the bars in descending order based on the count.
2. Orient the plot so that it is easy to read the labels.

```{r}

dictionary_small %>% 
  count(`Associated Domain`) %>% 
  mutate(`Associated Domain` = fct_reorder(`Associated Domain`,n)) %>% 
  ggplot(aes(x = `Associated Domain`, y = n)) +
  geom_col() +
  labs(x = "Associated Domain", y = "Count") +
  coord_flip()

```

### Further exploration of basic properties


#### Check for a tidy data frame

In a tidy data set, each column is a variable or id and each row is an observation. It will take some work to determine whether the data set is tidy.

It was difficult to assess whether the full data frame was tidy because of the large number of columns with confusing names. We will do the analysis on the smaller data frame.

$\rightarrow$ How many observations are in the data set?

```{r}

nrow(df_small)

```

### Data cleaning

#### Rename variables

I used the data dictionary to create more readable names for the minimal set of variables. You should add in new names for the additional variables you included in the data set.

The data frame with renamed columns is called `df_hud`.

```{r}

#Add your new names to this list. If you add variable names, the order of the variables in the dictionay_small will change and you will need to reassign the numbers.

df_hud <- df_small %>% 
  rename(coc_number = dictionary_small$Variable[2],
    total_sheltered = dictionary_small$Variable[3],
total_unsheltered = dictionary_small$Variable[4],
total_homeless = dictionary_small$Variable[5],
total_population = dictionary_small$Variable[6],
total_female_population = dictionary_small$Variable[7],
total_population_0_19 = dictionary_small$Variable[8],
total_population_65_plus = dictionary_small$Variable[9],
total_black = dictionary_small$Variable[10],
total_asian = dictionary_small$Variable[11],
total_pacific_islander = dictionary_small$Variable[12],
total_latino_hispanic = dictionary_small$Variable[13],
share_bachelors_or_higher = dictionary_small$Variable[14],
share_high_school_grad = dictionary_small$Variable[15],
share_less_than_high_school = dictionary_small$Variable[16],
share_some_college = dictionary_small$Variable[17],
house_price_index_2009 = dictionary_small$Variable[18],
rate_unemployment = dictionary_small$Variable[19],
net_migration = dictionary_small$Variable[20],
HUD_unit_occupancy_rate = dictionary_small$Variable[21],
number_eviction = dictionary_small$Variable[22],
percentage_excessive_drinking = dictionary_small$Variable[23],
medicare_reimbursements_per_enrollee = dictionary_small$Variable[24],
average_summer_temperature = dictionary_small$Variable[25],
total_annual_precipitation = dictionary_small$Variable[26],
average_Jan_temperature = dictionary_small$Variable[27],
total_Jan_precipitation = dictionary_small$Variable[28],
gini_coefficient_2016 = dictionary_small$Variable[29],
poverty_rate = dictionary_small$Variable[30],
share_renters_2016 = dictionary_small$Variable[31],
share_overcrowded_units_2016 = dictionary_small$Variable[32],
percentage_owners_cost_burden_2016 = dictionary_small$Variable[33],
percentage_renters_severe_cost_burden_2016 = dictionary_small$Variable[34],
share_HUD_units = dictionary_small$Variable[35],
high_housing_density = dictionary_small$Variable[36],
share_built_before_1940  = dictionary_small$Variable[37],
utility_costs  = dictionary_small$Variable[38],
rental_vacancy_rate = dictionary_small$Variable[39],
proportion_one_person_households  = dictionary_small$Variable[40],
share_under_18_with_single_parent = dictionary_small$Variable[41],
share_veteran_status = dictionary_small$Variable[42],
log_median_rent = dictionary_small$Variable[43],
migration_4_year_change  = dictionary_small$Variable[44],
share_no_bachelors = dictionary_small$Variable[45],
city_or_urban = dictionary_small$Variable[46],
suburban = dictionary_small$Variable[47]
)

```

$\rightarrow$ Examine the head of the new data frame with the updated names:

```{r}

names(df_hud)

```


$\rightarrow$ Display the names of the columns of the new data frame:

```{r}

skim_without_charts(df_hud)

```

### Identify and deal with missing values

$\rightarrow$ How many missing values are there in each column? Give the number of missing values and the percent of values in each column that are missing.

```{r}

df_hud <- df_hud %>% 
  filter(is.na(total_homeless) == FALSE)

skim_without_charts(df_hud)

```

We are interested in predicting the number of people experiencing homelessness. So, we will remove the rows where those numbers are missing.

$\rightarrow$ Remove the rows where `total_homeless` is `NA`.

```{r}

df_hud <- df_hud %>% 
  filter(is.na(total_homeless) == FALSE)

skim_without_charts(df_hud)

```

There are some variables that are missing many of their values. 

$\rightarrow$ Produce scatter plots of the variables that are missing many values vs. time to see if data are missing from particular years.

```{r}

df_hud %>% 
  ggplot(aes(x = year, y = migration_4_year_change)) +
  geom_point()

```

All of the data points in the scatter plot seem to fall onto the year 2017.

$\rightarrow$ Produce a data frame with data only from 2017. Call it `df_2017`.

```{r}

df_2017 <- df_hud %>% 
  filter(year == 2017)

```

$\rightarrow$ Check for missing values in the 2017 data.

```{r}

skim_without_charts(df_2017)

```

## Exploratory data analysis

We have two main goals when doing exploratory data analysis. The first is that we want to understand the data set more completely. The second goal is to explore relationships between the variables to help guide the modeling process to answer our specific question.

### Graphical summaries

####  Look at the distributions of the homeless counts

$\rightarrow$ Make a histogram of the total number of homeless in 2017.

```{r}

df_2017 %>% 
  ggplot(aes(x = total_homeless)) +
  geom_histogram(boundary = 0) +
  labs(x = "Total homeless", y = "Count")

```

There are some outliers in the histogram for the total homeless, making it positively skewed.

$\rightarrow$ Make a histogram of the number of sheltered homeless in 2017.

```{r}

df_2017 %>% 
  ggplot(aes(x = total_sheltered)) +
  geom_histogram(boundary = 0) +
  labs(x = "Sheltered homeless", y = "Count") 

```

There are some outliers in the histogram for the sheltered homeless, making it positively skewed.

$\rightarrow$ Make a histogram of the number of unsheltered homeless in 2017.

```{r}

df_2017 %>% 
  ggplot(aes(x = total_unsheltered)) +
  geom_histogram(boundary = 0) +
  labs(x = "Total population", y = "Count")

```

The histogram of the count of total population of each state is positively skewed.

Converting the total counts to rates relative to population size should remove extreme outliers.

$\rightarrow$ Use the `mutate` function to create a new variable `rate_homeless` that is the total number of homeless per 10,000 people in the population and make a histogram of `rate_homeless`.

```{r}

df_2017 %>% 
  mutate(rate_homeless = total_homeless/(total_population/10000)) %>% 
  ggplot(aes(x = rate_homeless)) +
  geom_histogram(boundary = 0) +
  labs(x = "Homeless rate per 10,000", y = "Count") 

```

The histogram above is unimodal and positively skewed.

$\rightarrow$ Compare boxplots of `rate_homeless` and `total_homeless` to visually assess whether outliers are less extreme in `rate_homeless` than in `total_homeless`.

```{r}

#Boxplot of rate_homeless
p1 <- df_2017 %>% 
  mutate(rate_homeless = total_homeless/(total_population/10000)) %>% 
  ggplot() +
  geom_boxplot(aes(y = rate_homeless))

#Boxplot of total_homeless
p2 <- df_2017 %>% 
  ggplot() +
  geom_boxplot(aes(y = total_homeless))

#Display as two panels in one graph
grid.arrange(p1, p2, nrow = 1)

```

#### Data processing

We will add rates to the data frame. Following the HUD report, we will produce rates per 10,000 people.

$\rightarrow$ Use the `mutate` function to create new variables `rate_homeless`, `rate_sheltered`, and `rate_unsheltered` in the data frame `df_2017` that are the counts per 10,000 people in the population.

```{r}

df_2017 <- df_2017 %>% 
  mutate(rate_sheltered = total_sheltered/(total_population/10000),
         rate_unsheltered = total_unsheltered/(total_population/10000),
         rate_homeless = total_homeless/(total_population/10000))

```

We should note that the demographic variables (race, gender, age) are given as total counts. We will also convert these totals to percentages.


$\rightarrow$ Use the `mutate` function to create new demographics variables in the data frame `df_2017` that are percentages of the total population.

```{r}

df_2017 <- df_2017 %>% 
  mutate(percent_black = total_black/total_population, 
  percent_latino_hispanic = total_latino_hispanic/total_population, 
  percent_asian = total_asian/total_population, 
  percent_pacific_islander = total_pacific_islander/total_population, 
  percent_population_0_19 = total_population_0_19/total_population, 
  percent_population_65_plus = total_population_65_plus/total_population,
  percent_female_population = total_female_population/total_population)

```

#### Basic summary

$\rightarrow$ How many people were experiencing homelessness in 2017? How many were sheltered and how many were unsheltered?

```{r}

df_2017 %>% 
  select(total_homeless, total_unsheltered, total_sheltered) %>% 
  colSums()

```

$\rightarrow$ What are the minimum, maximum, mean, and median number of total, sheltered, and unsheltered homeless in 2017?

```{r}

df_2017 %>% 
  select(total_homeless, total_unsheltered, total_sheltered) %>% 
  pivot_longer(cols = c(total_homeless, total_unsheltered, total_sheltered), names_to = "variable", values_to = "count") %>% 
  group_by(variable) %>% 
  summarize(min = min(count), max = max(count), mean = round(mean(count),1), median = median(count)) 

```

### Correlations between numerical variables


$\rightarrow$ Plot the correlation coefficients between all pairs of numerical variables using the `corrplot` function.

```{r}

df_2017 %>% 
  select_if(is.numeric) %>% 
  select(-"year") %>% 
  cor(use = "pairwise.complete.obs") %>% 
  corrplot(tl.cex = 0.5, type = "lower")

```

There are some variables that are highly correlated with the rate of homeless. Some are obvious, such as the rate of sheltered homeless. Other correlations show the existence of environmental variables, such as the share of overcrowded units, that are correlated with the rate of homeless.


Note the high correlation among subsets of the input variables. We will want to remove redundant variables or make a transformation of the input variables to deal with the correlation before constructing a model.


Next, we will find the variables with the highest correlation with `rate_homeless`. First, create the correlation matrix. Then examine the row with `rate_homeless` to find its correlation with the other variables.

```{r}

M <- df_2017 %>% 
  select_if(is.numeric) %>% 
  cor(use = "pairwise.complete.obs")


round(M["rate_homeless",],2)

```


Find the variables where the absolute value of the correlation with `rate_homeless` is greater than 0.3.

```{r}

M["rate_homeless",abs(M["rate_homeless",]) > 0.3 & is.na(M["rate_homeless",]) == FALSE] %>% 
  abs() %>%
  round(2) %>% 
  sort(decreasing = T)

```

$\rightarrow$ Make a pairs plot with a subset of the variables with the highest magnitude correlations. Select a small enough group so that you can see the panels. Do not include variables that you will not use as predictors (e.g. `rate_unsheltered`, any demographic totals)

```{r}

df_2017 %>% 
  select("rate_homeless", "share_renters_2016", "share_overcrowded_units_2016", "total_Jan_precipitation", "house_price_index_2009", "percentage_owners_cost_burden_2016", "log_median_rent", "utility_costs", "gini_coefficient_2016") %>% 
  filter("coc_number" != "CA-613") %>% 
  ggpairs(progress = FALSE, lower = list(continuous = "cor"), upper = list(continuous = "points"))

```

$\rightarrow$ Create additional plots to further understand the data set. Describe your findings.

```{r}

df_2017 %>% 
  select("rate_homeless",'share_bachelors_or_higher', 'share_high_school_grad', 'share_less_than_high_school', 'share_some_college') %>% 
  filter("coc_number" != "CA-613") %>% 
  ggpairs(progress = FALSE, lower = list(continuous = "cor"), upper = list(continuous = "points"))

```

I created another pairs plot using the extra variables that I added in the data set. Looking at all the predictors, I could see that there is some pattern in some graphs, but mostly the data points are scattered in a large area.

## Model

We will multiple approaches to construct models that predict `rate_homeless`:

1. Use statistical significance to create a multiple linear regression model.
2. Best subset selection for a multiple linear regression model.
3. Lasso
4. Ridge regression
5. XGBoost

To compare the different approaches, we will use a training and testing split of the data set.

### Set up the data set for training and testing


#### Remove some variables

There are several variables that we do not want to include as predictors. We want to remove demographic totals, the year, the CoC number, and the other homeless rates that we are not predicting. You may have additional variables to remove to create the data set that contains only the response variable and the predictors that you want to use.

```{r}

variable_remove = c("total_homeless", "total_sheltered", "total_unsheltered", "total_black", "total_latino_hispanic", "total_asian", "total_pacific_islander", "total_population_0_19", "total_population_65_plus", "total_female_population", "year", "coc_number", "total_population", "rate_unsheltered", "rate_sheltered", "share_high_school_grad", "share_less_than_high_school", "share_some_college", "share_bachelors_or_higher")

df_model <- df_2017 %>% 
  select(-all_of(variable_remove))
names(df_model)

```


$\rightarrow$ How many input variables remain in the data set that you can use as predictors?

```{r}

ncol(df_model) - 1

```

#### Get train and test splits


We will split the data into training and testing sets, with 85% of the data kept for training.   

```{r}

#Do the split. Keep 85% for training. Use stratified sampling based on rate_homeless
set.seed(20)
split <- initial_split(df_model, prop = 0.85, strata = rate_homeless)

#Extract the training and testing splits
df_train <- training(split)
df_test <- testing(split)

```


$\rightarrow$ Check the sizes of the training and testing splits

```{r}

dim(df_train)
dim(df_test)

```

### Full regression model

#### Fit the model on the training data

$\rightarrow$ Use the training data to fit a multiple linear regression model to predict `rate_homeless` using all possible predictor variables.

```{r}

fit <- lm(rate_homeless ~ ., df_train)

```

$\rightarrow$ Examine the summary of the fit. Which predictors have statistically significant coefficients? Do the signs of the coefficients make sense?

```{r}

summary(fit)

```

```{r}

#Name the summary
s <- summary(fit)

#Use logical indexing on the matrix of coefficients to find coefficients where the p-value < 0.05
s$coefficients[s$coefficients[,4] < 0.05,1] %>% 
  round(2) %>%     #Round because we don't need to look at a million decimal places
  data.frame()     #Create a data frame because it produces a nicer display

```

We should also do a qualitative assessment of the fit to the training data.

$\rightarrow$ Plot the residuals to look for systematic patterns of residuals that might suggest a new model.

```{r}

plot(fit,1)

```

The residual plot has a systematic pattern, which suggest that we should use a different model to predict the homelessness rate.

#### Assess the model on the testing data

$\rightarrow$ Use the model to predict the homeless rate in the testing data.

```{r}

lm_pred <- predict(fit, df_test)

```

$\rightarrow$ Make a scatter plot to compare the actual value and the predicted value of `rate_homeless`.

```{r}

plot(df_test$rate_homeless, lm_pred, xlab = "Measured homeless rate", ylab = "Predicted homeless rate", pch = 20)

```

$\rightarrow$ Compute the RMSE

```{r}

lm_RMSE <- sqrt(mean((df_test$rate_homeless- lm_pred)^2))

lm_RMSE

```

$\rightarrow$ Repeat the regression analysis, but using a different random seed before the train-test split. Does the train-test split affect the significance of the coefficients and their values?

```{r}

lm_RMSE <- (df_test$rate_homeless- lm_pred)^2 %>% 
  mean() %>% 
  sqrt()

lm_RMSE

```

### Subset selection

The full model contains too many predictors, so we will use subset selection on the training data to find a smaller number of predictors.


$\rightarrow$ Get the number of variables in the data set that will be used as predictors. This will be used to set the subset size.

```{r}

num_var <- ncol(df_train) - 1

```

$\rightarrow$ Do best subset selection on the training data. Set the maximum number of variables to equal the number of predictor variables in the data set.

```{r}

regfit_full <- regsubsets(rate_homeless ~ . , data = df_train, nvmax = num_var)

```

$\rightarrow$ Get the summary. We will use this after performing cross validation to determine the best model.

```{r}

reg_summary <- summary(regfit_full)

```

#### Cross validation

$\rightarrow$ Use 10-fold cross validation on the training data to determine the best subset of predictors in the model.

```{r}

#Number of observations
n <- nrow(df_train)

#number of folds
k <- 10

#folds
set.seed(1)
#Each observation is randomly assigned to a fold 1, 2, .. k
folds <- sample(1:k,n,replace = TRUE)

#Initialize error for testing data
cv_errors <- matrix(NA,k,num_var,dimnames = list(NULL,paste(1:num_var)))

#For each testing fold
for (j in 1:k){
  #Best subsets on training folds
  reg_fit_best <- regsubsets(rate_homeless ~ ., data = df_train[folds !=j,], nvmax = num_var)
  
  #test matrix with test fold
  test_mat <- model.matrix(rate_homeless ~ .,data = df_train[folds == j,])

    #Compare errors across models with different numbers of variables  
    for (i in 1:num_var){
    #get coefficients of best model with i variables
    coefi <- coef(reg_fit_best,id = i)
    #make predictions using test data
    pred <- test_mat[,names(coefi)] %*% coefi
    #compute test MSE
    cv_errors[j,i] <- mean((df_train$rate_homeless[folds == j] - pred)^2)
  }
}

#Average error over folds
cv_errors_mean <- apply(cv_errors,2,mean)

```

$\rightarrow$ Plot the assessment measures vs. the number of predictors

```{r}

par(mfrow = c(2,2))

ind_cp = which.min(reg_summary$cp)
plot(reg_summary$cp,type = "b",xlab = "Number of variables",ylab = "Cp", main = toString(ind_cp))
points(ind_cp, reg_summary$cp[ind_cp],col = "red",pch = 20)

ind_bic = which.min(reg_summary$bic)
plot(reg_summary$bic,type = "b",xlab = "Number of variables",ylab = "BIC", main = toString(ind_bic))
points(ind_bic, reg_summary$bic[ind_bic],col = "red",pch = 20)

ind_adjr2 = which.max(reg_summary$adjr2)
plot(reg_summary$adjr2,type = "b",xlab = "Number of variables",ylab = "Adjusted R2", main = toString(ind_adjr2))
points(ind_adjr2, reg_summary$adjr2[ind_adjr2],col = "red",pch = 20)

ind_cv <- which.min(cv_errors_mean)
plot(cv_errors_mean,type = "b",xlab = "Number of variables",ylab = "Cross validation", main = toString(ind_cv))
points(ind_cv, cv_errors_mean[ind_cv],col = "red",pch = 20)

```

Looking at the four measures, each measures have different numbers of variables that has the best model.  

$\rightarrow$ What are the coefficients of the best model according to different measures?

```{r}

coef(regfit_full,ind_cp) %>% 
  round(2) %>% 
  data.frame()

```

```{r}

coef(regfit_full,ind_bic) %>% 
  round(2) %>% 
  data.frame()

```

```{r}

coef(regfit_full,ind_adjr2) %>% 
  round(2) %>% 
  data.frame()

```


```{r}

coef(regfit_full,ind_cv) %>% 
  round(2) %>% 
  data.frame()

```


#### Assess the performance of the model on the testing data
 
Use the model to predict the homeless rate in the testing data. We will use the best CV model as an example, but you should compare the performance of all models.


$\rightarrow$ Select the variables that are indicated by the best subset selection criterion that you choose (4, BIC, Adjusted R-squared, or CV) and produce a new, smaller data frame.

```{r}

df_cv <- df_train %>% 
  select(all_of(names(coef(regfit_full,ind_cv))[2:length(coef(regfit_full,ind_cv))]), "rate_homeless")

head(df_cv)

```

$\rightarrow$ Fit the model using this subset. The `lm` object will help us to make predictions easily.

```{r}

fit_cv <- lm(rate_homeless ~., data = df_cv)

summary(fit_cv)

```

All of the coefficients in the model have a p-value of < .05, so they are all statistically significant.

$\rightarrow$ Generate predictions of the ACT score in the testing data.

```{r}

cv_pred <- predict(fit_cv, df_test)

```


$\rightarrow$ Make a scatter plot to compare the actual value and the predicted value of `rate_homeless`.

```{r}

plot(df_test$rate_homeless, cv_pred, xlab = "Measured homeless rate", ylab = "Predicted homeless rate", pch = 20)

```

$\rightarrow$ Compute the RMSE. How does the RMSE compare to the error for the full model?

```{r}

cv_rmse <- sqrt(mean((df_test$rate_homeless- cv_pred)^2))

cv_rmse

```

The RMSE on the Cross-Validation Model does better than than the Full Linear Regression Model's RMSE which was 11.46046.

### Lasso

The lasso is another approach to producing a linear regression model with a subset of the available predictors. The lasso works by finding coefficients $\boldsymbol{\beta}$ that minimize the cost function:

$$C(\boldsymbol{\beta}) = \sum_{i=1}^n(y_i - \beta_{0} - \sum_{j=1}^p\beta_{j}x_{ij})^2 + \lambda \sum_{j=1}^p|\beta_{j}| = \text{RSS} + \lambda \sum_{j=1}^p|\beta_{j}|$$

where $\lambda \geq 0$ is a tuning parameter (or hyperparameter).

$\rightarrow$ Prepare the data by creating a model matrix `x_train` from the training data. Create the model matrix using `model.matrix` so that it includes the training data for all predictors, but does not include a column for the intercept. Also create the training response data `y_train`.

```{r}

x_train <- model.matrix(rate_homeless ~ ., df_train)[,-1] #Remove the intercept
y_train <- df_train$rate_homeless

```

$\rightarrow$ Use cross-validation to find the best hyperparameter $\lambda$

```{r}

#alpha = 1 says that we will use the lasso
cv_lasso <- cv.glmnet(x_train, y_train, alpha = 1)

```


$\rightarrow$ Show error as a function of the hyperparameter $\lambda$ and its best value.

```{r}

plot(cv_lasso)

```

```{r}

best_lam <- cv_lasso$lambda.min
best_lam

```

$\rightarrow$ Fit the lasso with the best $\lambda$ using the function `glmnet`.

```{r}

lasso_mod <- glmnet(x_train, y_train, alpha = 1, lambda = best_lam)

```

$\rightarrow$ Examine the coefficients. Which variables have non-zero coefficients?

```{r}

round(coef(lasso_mod),3)

```

#### Look at prediction error

$\rightarrow$ Use the model to predict the homeless rate in the testing data.

```{r}

x_test <- model.matrix(rate_homeless ~ .,df_test)[,-1] #Remove the intercept
lasso_pred <- predict(lasso_mod, s = best_lam, newx = x_test)

```

$\rightarrow$ Make a scatter plot to compare the actual value and the predicted value of `rate_homeless`.

```{r}

plot(df_test$rate_homeless, lasso_pred, xlab = "Measured homeless rate", ylab = "Predicted homeless rate", pch = 20)

```

$\rightarrow$ Compute the RMSE. How does it compare to the other models?

```{r}

lasso_rmse <- sqrt(mean((df_test$rate_homeless - lasso_pred)^2))
lasso_rmse

```

The Lasso RMSE does pretty well in lowering the variance between all the data points compared to the other models which are slightly higher.

### Ridge regression

Ridge regression is another approach to model building 

Ridge regression works by finding coefficients $\boldsymbol{\beta}$ that minimize the cost function:

$$C(\boldsymbol{\beta}) = \sum_{i=1}^n(y_i - \beta_{0} - \sum_{j=1}^p\beta_{j}x_{ij})^2 + \lambda \sum_{j=1}^p\beta_{j}^2 = \text{RSS} + \lambda \sum_{j=1}^p\beta_{j}^2$$

In contrast to the lasso, ridge regression will not reduce the number of non-zero coefficients in the model. Ridge regression will shrink the coefficients, which helps to prevent overfitting of the training data.

The fitting procedure for the ridge regression model mirrors the lasso approach, only changing the parameter $\alpha$ to 0 in the `cv.glmnet` and `glmnet` functions.


$\rightarrow$ Fit and assess a ridge regression model. How does it compare to the other models?

First, do cross validation to find the best value of the λ

```{r}

cv_ridge <- cv.glmnet(x_train,y_train,alpha = 0)

best_lam <- cv_ridge$lambda.min

```

$\rightarrow$ Fit the model with the best value of the λ

```{r}

ridge_mod <- glmnet(x_train, y_train, alpha = 0, lambda = best_lam)

```
$\rightarrow$ Predict the ACT scores in the test data

```{r}

ridge_pred <- predict(ridge_mod, s = best_lam, newx = x_test)

```

$\rightarrow$ Compute the RMSE

```{r}

ridge_rmse <- sqrt(mean((df_test$rate_homeless - ridge_pred)^2))
ridge_rmse

```

The RMSE of Ridge Regression has a higher variance then Lasso Regression and the Cross-Validation Model, but is lower than the Full Regression model.

### XGBoost

XGBoost is short for eXtreme Gradient Boosting.

We are going to use the `tidymodels` package to fit the XGBoost model.

#### Set up the model

The model will be a boosted tree model, so we start by specifying the features of a `boost_tree` model. The`boost_tree` creates a specification of a model, but does not fit the model.

```{r}

xgb_spec <- boost_tree(
  mode = "regression",  #We are solving a regression problem
  trees = 1000, 
  tree_depth = tune(),  # tune() says that we will specify this parameter later
  min_n = tune(), 
  loss_reduction = tune(),                     
  sample_size = tune(), 
  mtry = tune(),         
  learn_rate = tune(),                         
  ) %>% 
  set_engine("xgboost", objective = "reg:squarederror") ## We will use xgboost to fit the model and try to minimize the squared error

xgb_spec

```

Create a workflow that specifies the model formula and the model type. We are still setting up the model; this does not fit the model.

```{r}

xgb_wf <- workflow() %>%
  add_formula(rate_homeless ~ .) %>%
  add_model(xgb_spec)

xgb_wf

```

#### Fit the model

We need to fit all of the parameters that we specified as `tune()`. We will specify the parameter grid using the functions `grid_latin_hypercube`:

```{r}
xgb_grid <- grid_latin_hypercube(
  tree_depth(),
  min_n(),
  loss_reduction(),
  sample_size = sample_prop(),
  finalize(mtry(), df_train),
  learn_rate(),
  size = 30  #Create 30 sets of the 6 parameters
)

```


Create folds for cross-validation. 

```{r}

folds <- vfold_cv(df_train)

#Can also try using stratified sampling based on rate_homeless
#folds <- vfold_cv(df_train, strata = rate_homeless)

```


Do the parameter fitting. This will take some time.

```{r}

xgb_res <- tune_grid(
  xgb_wf,              #The workflow
  resamples = folds,   #The training data split into folds
  grid = xgb_grid,     #The grid of parameters to fit
  control = control_grid(save_pred = TRUE)
)

xgb_res

```

Set up the final workflow with the best model parameters.

```{r}

#Get the best model, according to RMSE
best_rmse <- select_best(xgb_res, "rmse")

#Update the workflow with the best parameters
final_xgb <- finalize_workflow(
  xgb_wf,
  best_rmse
)

final_xgb

```

#### Prediction

Fit the final model to the training data and predict the test data.
```{r}

final_res <- last_fit(final_xgb, split)

```

Show the RMSE. Compare the result to the test RMSE for the other models.

```{r}

collect_metrics(final_res)

```

The RMSE of the tree model has a much higher variance the the Best Subset, Lasso, and Ridge Model, but lower variance than the Linear Regression Model.

Plot the ACT scores and the prediction

```{r}

plot(final_res$.predictions[[1]]$rate_homeless,final_res$.predictions[[1]]$.pred, pch = 20, xlab = "Homeless rate", ylab = "Predicted homeless rate")

```


#### Relative importance of predictors

Look at which predictors are most important in the model

```{r}

final_xgb %>%
  fit(data = df_train) %>%
  pull_workflow_fit() %>%
  vip(geom = "col")

```



### Compare models

You used several methods to construct a model

1. Use statistical significance to create a multiple linear regression model.
2. Best subset selection for a multiple linear regression model.
3. Lasso
4. Ridge regression
5. XGBoost

Compare the performance of the models. 

## Additional step

In addition to completing the above analyses, you should perform any revisions to the models that you think might improve their performance. Consult Canvas for further directions.

For the additional step, I've decided to add more variables to the model to see if it would lower the RMSE and improve the overall prediction of the model.

```{r}

#Removing some variables
variable_remove = c("total_homeless", "total_sheltered", "total_unsheltered", "total_black", "total_latino_hispanic", "total_asian", "total_pacific_islander", "total_population_0_19", "total_population_65_plus", "total_female_population", "year", "coc_number", "total_population", "rate_unsheltered", "share_no_bachelors", "share_some_college")

df_model <- df_2017 %>% 
  select(-all_of(variable_remove))
names(df_model)

```

```{r}

#Do the split. Keep 85% for training. Use stratified sampling based on rate_homeless
set.seed(20)
split <- initial_split(df_model, prop = 0.85, strata = rate_homeless)

#Extract the training and testing splits
df_train <- training(split)
df_test <- testing(split)


#Checking the train test split sizes
dim(df_train)
dim(df_test)

```

Using the training data to fit a multiple linear regression model to predict `rate_homeless` using all possible predictor variables.
```{r}

fit <- lm(rate_homeless ~ ., df_train)

summary(fit)

```

Plotting the Residual to see if there is any systematic pattern.
```{r}

plot(fit,1)

```

Looking at the fit, the new model is a better improvement than the original model, but there is still some slight pattern.

```{r}

lm_pred <- predict(fit, df_test)

```

$\rightarrow$ Make a scatter plot to compare the actual value and the predicted value of `rate_homeless`.

```{r}

plot(df_test$rate_homeless, lm_pred, xlab = "Measured homeless rate", ylab = "Predicted homeless rate", pch = 20)

```

```{r}

lm_rmse <- sqrt(mean((df_test$rate_homeless- lm_pred)^2))

lm_rmse

```

When adding the three extra education variables, linear regression RMSE value lowered by two to 9.37 which is a considerable change from the original model that was 11.46.

$\rightarrow$ Get the number of variables in the data set that will be used as predictors. This will be used to set the subset size.

```{r}

num_var <- ncol(df_train) - 1

```

$\rightarrow$ Doing best subset selection on the training data. Set the maximum number of variables to equal the number of predictor variables in the data set.

```{r}

regfit_full <- regsubsets(rate_homeless ~ . , data = df_train, nvmax = num_var)

```

$\rightarrow$ Getting the summary. We will use this after performing cross validation to determine the best model.

```{r}

reg_summary <- summary(regfit_full)

```

#### Cross validation

$\rightarrow$ Using 10-fold cross validation on the training data to determine the best subset of predictors in the model.

```{r}

#Number of observations
n <- nrow(df_train)

#number of folds
k <- 10

#folds
set.seed(1)
#Each observation is randomly assigned to a fold 1, 2, .. k
folds <- sample(1:k,n,replace = TRUE)

#Initialize error for testing data
cv_errors <- matrix(NA,k,num_var,dimnames = list(NULL,paste(1:num_var)))

#For each testing fold
for (j in 1:k){
  #Best subsets on training folds
  reg_fit_best <- regsubsets(rate_homeless ~ ., data = df_train[folds !=j,], nvmax = num_var)
  
  #test matrix with test fold`    
  test_mat <- model.matrix(rate_homeless ~ .,data = df_train[folds == j,])

    #Compare errors across models with different numbers of variables  
    for (i in 1:num_var){
    #get coefficients of best model with i variables
    coefi <- coef(reg_fit_best,id = i)
    #make predictions using test data
    pred <- test_mat[,names(coefi)] %*% coefi
    #compute test MSE
    cv_errors[j,i] <- mean((df_train$rate_homeless[folds == j] - pred)^2)
  }
}

#Average error over folds
cv_errors_mean <- apply(cv_errors,2,mean)

```

$\rightarrow$ Plotting the assessment measures vs. the number of predictors

```{r}

par(mfrow = c(2,2))

ind_cp = which.min(reg_summary$cp)
plot(reg_summary$cp,type = "b",xlab = "Number of variables",ylab = "Cp", main = toString(ind_cp))
points(ind_cp, reg_summary$cp[ind_cp],col = "red",pch = 20)

ind_bic = which.min(reg_summary$bic)
plot(reg_summary$bic,type = "b",xlab = "Number of variables",ylab = "BIC", main = toString(ind_bic))
points(ind_bic, reg_summary$bic[ind_bic],col = "red",pch = 20)

ind_adjr2 = which.max(reg_summary$adjr2)
plot(reg_summary$adjr2,type = "b",xlab = "Number of variables",ylab = "Adjusted R2", main = toString(ind_adjr2))
points(ind_adjr2, reg_summary$adjr2[ind_adjr2],col = "red",pch = 20)

ind_cv <- which.min(cv_errors_mean)
plot(cv_errors_mean,type = "b",xlab = "Number of variables",ylab = "Cross validation", main = toString(ind_cv))
points(ind_cv, cv_errors_mean[ind_cv],col = "red",pch = 20)

```

```{r}

coef(regfit_full,ind_cv) %>% 
  round(2) %>% 
  data.frame()

```

```{r}

fit_cv <- lm(rate_homeless ~., data = df_cv)

summary(fit_cv)

```

All of the coefficients in the model have a p-value of < .05, so they are all statistically significant.

$\rightarrow$ Generate predictions of the ACT score in the testing data.

```{r}

cv_pred <- predict(fit_cv, df_test)

```


$\rightarrow$ Make a scatter plot to compare the actual value and the predicted value of `rate_homeless`.

```{r}

plot(df_test$rate_homeless, cv_pred, xlab = "Measured homeless rate", ylab = "Predicted homeless rate", pch = 20)

```

$\rightarrow$ Compute the RMSE. How does the RMSE compare to the error for the full model?

```{r}

cv_rmse <- sqrt(mean((df_test$rate_homeless- cv_pred)^2))

cv_rmse

```

The RMSE of cross-validation model did not change because it didn't consider the education variables as significant.

$\rightarrow$ Preparing the data by creating a model matrix `x_train` from the training data

```{r}

x_train <- model.matrix(rate_homeless ~ ., df_train)[,-1] #Remove the intercept
y_train <- df_train$rate_homeless

```

$\rightarrow$ Use cross-validation to find the best hyperparameter $\lambda$

```{r}

#alpha = 1 says that we will use the lasso
cv_lasso <- cv.glmnet(x_train, y_train, alpha = 1)

```


$\rightarrow$ Show error as a function of the hyperparameter $\lambda$ and its best value.

```{r}

plot(cv_lasso)

```

```{r}

best_lam <- cv_lasso$lambda.min
best_lam

```

$\rightarrow$ Fit the lasso with the best $\lambda$ using the function `glmnet`.

```{r}

lasso_mod <- glmnet(x_train, y_train, alpha = 1, lambda = best_lam)

```

$\rightarrow$ Examine the coefficients. Which variables have non-zero coefficients?

```{r}

round(coef(lasso_mod),3)

```

$\rightarrow$ Use the model to predict the homeless rate in the testing data.

```{r}

x_test <- model.matrix(rate_homeless ~ .,df_test)[,-1] #Remove the intercept
lasso_pred <- predict(lasso_mod, s = best_lam, newx = x_test)

```

$\rightarrow$ Make a scatter plot to compare the actual value and the predicted value of `rate_homeless`.

```{r}

plot(df_test$rate_homeless, lasso_pred, xlab = "Measured homeless rate", ylab = "Predicted homeless rate", pch = 20)

```

$\rightarrow$ Compute the RMSE. How does it compare to the other models?

```{r}

lasso_RMSE <- sqrt(mean((df_test$rate_homeless - lasso_pred)^2))
lasso_RMSE

```

The RMSE of Lasso Regression for the new model lowered by around .6 which is a slight improvement from the original model which was 8.632.

$\rightarrow$ Fit and assess a ridge regression model. How does it compare to the other models?

First, do cross validation to find the best value of the λ

```{r}

cv_ridge <- cv.glmnet(x_train,y_train,alpha = 0)

best_lam <- cv_ridge$lambda.min

```

$\rightarrow$ Fit the model with the best value of the λ

```{r}

ridge_mod <- glmnet(x_train, y_train, alpha = 0, lambda = best_lam)

```
$\rightarrow$ Predict the ACT scores in the test data

```{r}

ridge_pred <- predict(ridge_mod, s = best_lam, newx = x_test)

```

$\rightarrow$ Compute the RMSE

```{r}

ridge_rmse <- sqrt(mean((df_test$rate_homeless - ridge_pred)^2))
ridge_rmse

```

The RMSE of new Ridge Regression model has a slightly higher variance than the first model which was 9.47

#### Setting up the model

The model will be a boosted tree model, so we start by specifying the features of a `boost_tree` model. The`boost_tree` creates a specification of a model, but does not fit the model.

```{r}

xgb_spec <- boost_tree(
  mode = "regression",  #We are solving a regression problem
  trees = 1000, 
  tree_depth = tune(),  # tune() says that we will specify this parameter later
  min_n = tune(), 
  loss_reduction = tune(),                     
  sample_size = tune(), 
  mtry = tune(),         
  learn_rate = tune(),                         
  ) %>% 
  set_engine("xgboost", objective = "reg:squarederror") ## We will use xgboost to fit the model and try to minimize the squared error

xgb_spec

```

Create a workflow that specifies the model formula and the model type.

```{r}

xgb_wf <- workflow() %>%
  add_formula(rate_homeless ~ .) %>%
  add_model(xgb_spec)

xgb_wf

```

We need to fit all of the parameters that we specified as `tune()`. We will specify the parameter grid using the functions `grid_latin_hypercube`:

```{r}
xgb_grid <- grid_latin_hypercube(
  tree_depth(),
  min_n(),
  loss_reduction(),
  sample_size = sample_prop(),
  finalize(mtry(), df_train),
  learn_rate(),
  size = 30  #Create 30 sets of the 6 parameters
)

```

Create folds for cross-validation. 

```{r}

folds <- vfold_cv(df_train)

#Can also try using stratified sampling based on rate_homeless
#folds <- vfold_cv(df_train, strata = rate_homeless)

```

Do the parameter fitting. This will take some time.

```{r}

xgb_res <- tune_grid(
  xgb_wf,              #The workflow
  resamples = folds,   #The training data split into folds
  grid = xgb_grid,     #The grid of parameters to fit
  control = control_grid(save_pred = TRUE)
)

xgb_res

```

Set up the final workflow with the best model parameters.

```{r}

#Get the best model, according to RMSE
best_RMSE <- select_best(xgb_res, "rmse")

#Update the workflow with the best parameters
final_xgb <- finalize_workflow(
  xgb_wf,
  best_RMSE
)

final_xgb

```

Fit the final model to the training data and predict the test data.

```{r}

final_res <- last_fit(final_xgb, split)

```

Show the RMSE. Compare the result to the test RMSE for the other models.

```{r}

collect_metrics(final_res)

```

The new XGBoost model RMSE was 8.305 which is lower by around one. This is better than the original model which was 9.226.

Plot the ACT scores and the prediction

```{r}

plot(final_res$.predictions[[1]]$rate_homeless,final_res$.predictions[[1]]$.pred, pch = 20, xlab = "Homeless rate", ylab = "Predicted homeless rate")

```

## Conclusion

In the introduction, we wanted to investigate whether there are alternative modeling approaches that outperform the models described in the HUD report. We wanted to see which variables in the homelessness data set that would have impact the performance in our model. First loaded in the data HUD TO3-05b Analysis File and 05b_analysis_file_update.csv to the program so that we can use it for our project. Then, we pre-processed the data so that we only looked at the data in 2017 since there was only data from that year. Then we constructed a list of variables that we would think is important to our model. After filtering out the data to only include the variables in our list, we would then split the data into a testing and training data to evaluate how well our model would work. First we collected the Residual Squared Mean Error (RMSE) from a linear regression which was 11.46. Then we used Lasso (8.632) and Ridge (8.794) regression to see if we could improve the RMSE any further. The RMSE of all these model was pretty high since our prediction would be 8-11 points off from the actual values. This could be improved, so I decided to add some more variables in the additional step. Some additional variables were share of high school grads, less than high school grad, and college bachelors. Repeating all the regression model with the three new added variables, we found that it decreased the RMSE on all of the models. It decreased Full Regression's RMSE from 11.46 to 9.36, Boost's RMSE from 8.794 to 8.305, and Lasso's RMSE lowered from 8.791 to 8.077. It didn't change the cross-validation model's RMSE, and increased Ridge's RMSE from 9.47 to 9.70. Overall, this is pretty substantial considering that we added only three extra variables. If we added in any more variables the the model, then we could definitely improve it until we can get to what we could be satisfied with. There were some limitations in the data that could have helped us improve our model if it exist. We only looked at the data for 2017, and if we could have more in the later years it could help us improve our model prediction.

After completing your analyses, you will make your conclusions and communicate your results. Consult Canvas for further directions.


